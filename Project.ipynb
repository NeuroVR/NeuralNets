{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch as t\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as tsf\n",
    "import scipy.misc\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_links = list()\n",
    "meme_links.append('/meme/First-World-Problems?sort=top-2019')\n",
    "meme_links.append('/meme/Grandma-Finds-The-Internet?sort=top-2019')\n",
    "meme_links.append('/meme/Success-Kid?sort=top-2019')\n",
    "meme_links.append('/meme/Leonardo-Dicaprio-Cheers?sort=top-2019')\n",
    "meme_links.append('/meme/That-Would-Be-Great?sort=top-2019')\n",
    "meme_links.append('/meme/Creepy-Condescending-Wonka?sort=top-2019')\n",
    "meme_links.append('/meme/Disaster-Girl?sort=top-2019')\n",
    "meme_links.append('/meme/Futurama-Fry?sort=top-2019')\n",
    "meme_links.append('/meme/One-Does-Not-Simply?sort=top-2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = ['boromir_memes/'+'First-World-Problems.jpg','boromir_memes/Grandma-Finds-The-Internet.jpg','boromir_memes/Success-Kid.jpg',\n",
    "              'boromir_memes/'+'Leonardo-Dicaprio-Cheers.jpg','boromir_memes/That-Would-Be-Great.jpg','boromir_memes/'+'Creepy-Condescending-Wonka.jpg',\n",
    "              'boromir_memes/'+'Disaster-Girl.jpg','boromir_memes/'+'Futurama-Fry.jpg','boromir_memes/'+'One-Does-Not-Simply.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for name in image_names:\n",
    "    input_image = Image.open(name)\n",
    "    preprocess = tsf.Compose([\n",
    "        tsf.ToTensor(),\n",
    "        tsf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) \n",
    "    \n",
    "    images.append(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping meme name to image\n",
    "desc =list()\n",
    "desc.append(('First World Problems',images[0]))\n",
    "desc.append(('Grandma Finds The Internet',images[1]))\n",
    "desc.append(('Success Kid',images[2]))\n",
    "desc.append(('This is to all',images[3]))\n",
    "desc.append(('That would be great',images[4]))\n",
    "desc.append(('Creepy Condescending Wonka',images[5]))\n",
    "desc.append(('Disaster Girl',images[6]))\n",
    "desc.append((\"NOT SURE IF\",images[7]))\n",
    "desc.append((\"ONE DOES NOT SIMPLY\",images[8]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 links proccessed\n",
      "10 links proccessed\n",
      "20 links proccessed\n",
      "30 links proccessed\n",
      "40 links proccessed\n",
      "50 links proccessed\n",
      "60 links proccessed\n",
      "70 links proccessed\n",
      "80 links proccessed\n",
      "90 links proccessed\n",
      "100 links proccessed\n",
      "110 links proccessed\n",
      "120 links proccessed\n",
      "130 links proccessed\n",
      "140 links proccessed\n",
      "150 links proccessed\n",
      "160 links proccessed\n",
      "170 links proccessed\n",
      "180 links proccessed\n",
      "190 links proccessed\n",
      "200 links proccessed\n",
      "210 links proccessed\n",
      "220 links proccessed\n",
      "230 links proccessed\n",
      "240 links proccessed\n",
      "250 links proccessed\n",
      "260 links proccessed\n",
      "270 links proccessed\n",
      "280 links proccessed\n",
      "290 links proccessed\n",
      "300 links proccessed\n",
      "310 links proccessed\n",
      "320 links proccessed\n",
      "330 links proccessed\n",
      "340 links proccessed\n",
      "350 links proccessed\n",
      "360 links proccessed\n",
      "370 links proccessed\n",
      "380 links proccessed\n",
      "390 links proccessed\n",
      "400 links proccessed\n",
      "410 links proccessed\n",
      "420 links proccessed\n",
      "430 links proccessed\n",
      "440 links proccessed\n",
      "450 links proccessed\n",
      "460 links proccessed\n",
      "470 links proccessed\n",
      "480 links proccessed\n",
      "490 links proccessed\n",
      "500 links proccessed\n",
      "510 links proccessed\n",
      "520 links proccessed\n",
      "530 links proccessed\n",
      "540 links proccessed\n",
      "550 links proccessed\n",
      "560 links proccessed\n",
      "570 links proccessed\n",
      "580 links proccessed\n",
      "590 links proccessed\n",
      "600 links proccessed\n",
      "610 links proccessed\n",
      "620 links proccessed\n",
      "630 links proccessed\n",
      "640 links proccessed\n",
      "650 links proccessed\n",
      "660 links proccessed\n",
      "670 links proccessed\n",
      "680 links proccessed\n",
      "690 links proccessed\n",
      "700 links proccessed\n",
      "710 links proccessed\n",
      "720 links proccessed\n",
      "730 links proccessed\n",
      "740 links proccessed\n",
      "750 links proccessed\n",
      "760 links proccessed\n",
      "770 links proccessed\n",
      "780 links proccessed\n"
     ]
    }
   ],
   "source": [
    "#parsing\n",
    "\n",
    "datasets = list()\n",
    "i = 0\n",
    "for link in meme_links:\n",
    "    dataset = list()\n",
    "\n",
    "    while link is not None:\n",
    "        html_1 = requests.get('https://imgflip.com'+link)\n",
    "        if i % 10 == 0:\n",
    "            print('{} links proccessed'.format(i))\n",
    "        i += 1\n",
    "        if html_1.status_code != 200:\n",
    "            print('ERROR')\n",
    "            break\n",
    "        soup = BeautifulSoup(html_1.text, 'html.parser')\n",
    "        lef = soup.find(\"div\", {\"id\": \"base-left\"})\n",
    "        img = lef.find_all('img')\n",
    "        units = lef.find_all(class_ ='base-unit clearfix')\n",
    "        for u in units:\n",
    "            if u.find('img')  and u.find('img')['alt'] != '':\n",
    "                alt_tag = u.find('img')['alt'].split('|')[1].strip()\n",
    "                dataset.append(alt_tag)\n",
    "        link = soup.find(class_ = 'pager-next l but')\n",
    "        if link:\n",
    "            link = link['href']\n",
    "\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_id = 0\n",
    "lines = list()\n",
    "for pic_id, dataset in enumerate(datasets):\n",
    "    for text in dataset:\n",
    "        lines.append((pic_id, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "import tqdm\n",
    "import progressbar\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "with open('glove.6B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        if line[0] not in all_words:\n",
    "            all_words.add(line[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = list()\n",
    "for line in lines:\n",
    "    line1 = re.sub(r'[^; \\w]+', '', line[1])\n",
    "    k = line1.find(';')\n",
    "    if k > 0:\n",
    "        line1 = line1[:k] +' ' + line1[k:]\n",
    "    fl = 0\n",
    "    for k in line1.split(' '):\n",
    "        if k.lower() not in all_words:\n",
    "            fl = 1\n",
    "    if not fl:\n",
    "        new_lines.append((line[0],line1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, sentences):\n",
    "        self.all_characters = set()\n",
    "        for line in sentences:\n",
    "            line = re.sub(r'[^; \\w]+', '', line[1])\n",
    "            for k in line.split(' '):\n",
    "                if k.lower() not in self.all_characters:\n",
    "                    self.all_characters.add(k.lower())\n",
    "        self.all_characters = list(sorted(self.all_characters))+['<eos>', '<go>']\n",
    "        self.char_to_id = {x[1]:x[0] for x in enumerate(self.all_characters)}\n",
    "        self.id_to_char = {x[0]:x[1] for x in enumerate(self.all_characters)}\n",
    "        self.size = len(self.all_characters)\n",
    "\n",
    "    def encode(self, line):\n",
    "        return [self.char_to_id[x] for x in line]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.id_to_char[x] for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('linesREALTOPnew3.pickle', 'rb') as f:\n",
    "    lines =  pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embs = dict()\n",
    "\n",
    "with open('glove.6B.300d.txt', 'rb') as f:\n",
    "    for l in f:\n",
    "        line = l.decode().split()\n",
    "        word = line[0]\n",
    "        if word in vocab.all_characters:\n",
    "            words_embs[vocab.char_to_id[word]] = torch.FloatTensor(np.array(line[1:]).astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
    "model.eval()\n",
    "model.fc = torch.nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "class EncoderRNN1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN1, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size * 2, n_layers, batch_first=False)\n",
    "        self.lin = nn.Linear(2048 + 300, hidden_size)\n",
    "        self.cnn = model\n",
    "    def forward(self, input_pair, hidden):\n",
    "        img = input_pair[1]\n",
    "        word_inputs = input_pair[0]\n",
    "        embedded = words_embs[word_inputs[0].item()]\n",
    "        seq_len = len(word_inputs)\n",
    "        for i in range(1, len(word_inputs)):\n",
    "            embedded = words_embs[word_inputs[i].item()] + embedded\n",
    "        embedded = embedded / seq_len\n",
    "        with torch.no_grad():\n",
    "            img = model(img)\n",
    "        embedded = torch.cat((img.squeeze(), embedded), 0)\n",
    "        embedded = embedded.view(1, 1, -1)\n",
    "        hidden = self.lin(embedded)\n",
    "        return  hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "class AttnDecoderRNN1(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN1, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size , n_layers, batch_first=False)\n",
    "        self.out = nn.Linear(hidden_size , output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, word_input,  last_hidden, l_c):\n",
    "        \n",
    "        global words_embs\n",
    "        if (len(words_embs.get(word_input.item(), torch.LongTensor([0]))) != 1):\n",
    "            word_embedded =words_embs[word_input.item()].view(1, 1, -1) # S=1 x B x N\n",
    "        else:\n",
    "            word_embedded = self.embedding(word_input).view(1, 1, -1)\n",
    "        \n",
    "        rnn_output, (hidden,c) = self.lstm(word_embedded, (last_hidden, l_c))\n",
    "\n",
    "        m = nn.Dropout(p=self.dropout_p)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        rnn_output = m(rnn_output)\n",
    "        output = F.log_softmax(self.out(rnn_output))\n",
    "        \n",
    "        return output, (hidden, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pair(line,vocab):\n",
    "    ans = list()\n",
    "    ans1 = list()\n",
    "    line11 = line[1]\n",
    "    ans.append(vocab.char_to_id['<go>'])\n",
    "    for k in line11.split(' '):\n",
    "        ans.append(vocab.char_to_id[k.lower()])\n",
    "    line1 = desc[line[0]][0]\n",
    "    img = desc[line[0]][1]\n",
    "    for k in line1.split(' '):\n",
    "        ans1.append(vocab.char_to_id[k.lower()])\n",
    "    ans.append(vocab.char_to_id['<eos>'])\n",
    "    ans =torch.LongTensor(ans)\n",
    "    ans1 = torch.LongTensor(ans1)\n",
    "\n",
    "    return ((ans1,img), ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'rb') as f:\n",
    "    model =  pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "n_layers = 1\n",
    "dropout_p = 0\n",
    "\n",
    "encoder = EncoderRNN1(vocab.size, hidden_size, n_layers)\n",
    "decoder = AttnDecoderRNN1(hidden_size, vocab.size, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay = 1e-5)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate,weight_decay = 1e-5)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.75\n",
    "clip = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "def train1(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, vocab):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 \n",
    "\n",
    "    input_length = input_variable[0].size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    decoder_input = torch.Tensor([vocab.char_to_id['<go>']]).long()\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    dec_c = Variable(torch.zeros(1, decoder.hidden_size )).view(1, 1, -1)\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, (decoder_hidden,dec_c) = decoder(decoder_input,  decoder_hidden,dec_c)\n",
    "            loss += criterion(decoder_output, target_variable[di].unsqueeze(0))\n",
    "            decoder_input = target_variable[di]\n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, (decoder_hidden,dec_c) = decoder(decoder_input,  decoder_hidden,dec_c)\n",
    "            loss += criterion(decoder_output, target_variable[di].unsqueeze(0))\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            if ni == vocab.char_to_id['<eos>']: break\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]])) # Chosen word is next input)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item()  / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "import time\n",
    "n_epochs = 30000000\n",
    "plot_every = 50\n",
    "print_every = 50\n",
    "\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 \n",
    "plot_loss_total = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "from tqdm import tqdm\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    training_pair = create_pair(random.choice(lines), vocab)\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    loss = train1(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,1000, vocab)\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0: continue\n",
    "    if epoch % 55555 == 0:\n",
    "        if teacher_forcing_ratio > 0.5:\n",
    "            teacher_forcing_ratio -= 0.1\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = ' (%d %d%%) %.4f' % ( epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('enctopREALTOPnew3.pickle', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "with open('dectopREALTOPnew3.pickle', 'rb') as f:\n",
    "    decoder= pickle.load(f)\n",
    "with open('vocabREALTOPnew3.pickle', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "with open('decoptREALTOPnew3.pickle', 'rb') as f:\n",
    "    decoder_optimizer = pickle.load(f)\n",
    "with open('encopttREALTOPnew3.pickle', 'rb') as f:\n",
    "    encoder_optimizer = pickle.load( f)\n",
    "with open('datasetsREALTOPnew3.pickle', 'rb') as f:\n",
    "    datasets =  pickle.load(f)\n",
    "with open('datasetsREALTOPnew3.pickle', 'rb') as f:\n",
    "    datasets =  pickle.load(f)\n",
    "with open('linesREALTOPnew3.pickle', 'rb') as f:\n",
    "    lines =  pickle.load(f) \n",
    "with open('model.pickle', 'rb') as f:\n",
    "    model =  pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(pic_id, encoder, decoder, encoder_optimizer, decoder_optimizer, max_length, vocab):\n",
    "    ans = list()\n",
    "\n",
    "    line1 = desc[pic_id][0]\n",
    "    for k in line1.split(' '):\n",
    "        ans.append(vocab.char_to_id[k.lower()])\n",
    "    ans =torch.LongTensor(ans)\n",
    "    img = desc[pic_id][1]\n",
    "\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_hidden = encoder((ans,img), encoder_hidden)\n",
    "    \n",
    "    decoder_input = torch.Tensor([vocab.char_to_id['<go>']]).long()\n",
    "    dec_c = Variable(torch.zeros(1, decoder.hidden_size )).view(1, 1, -1)\n",
    "    decoder_hidden =  encoder_hidden                                                          \n",
    "    result = str()\n",
    "    for di in range(1000):\n",
    "        decoder_output, (decoder_hidden,dec_c) = decoder(decoder_input,  decoder_hidden,dec_c)\n",
    "        hh = decoder_output[0].clone().detach().numpy()\n",
    "        sampler = torch.distributions.Categorical(torch.FloatTensor(np.exp(hh)))\n",
    "        ni = sampler.sample().data.numpy().item()\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if ni == vocab.char_to_id['<eos>']: break\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]])) \n",
    "        result += (vocab.id_to_char[ni]+' ')\n",
    "    print(result[5:].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF LIVE COULD STOP MAKING WIMP ; SKIP STUPID \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodion/miniconda3/envs/sphere-py37/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "pic_id = 4\n",
    "gen(pic_id, encoder, decoder, encoder_optimizer, decoder_optimizer, 1000, vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
